{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#what-is-magicflow","title":"What is MagicFlow?","text":"<p>MagicFlow is a Python-based automation framework designed to streamline idempotent daily operations and reduce the overhead of Business-As-Usual (BAU) tasks. It is built to integrate seamlessly into any API-driven self-service platform, providing flexible and scalable automation capabilities.</p> <p>This documentation provides:</p> <ul> <li>A general overview of MagicFlow's capabilities</li> <li>An introduction to the Jobs and Workflow frameworks that power its automation logic</li> </ul>"},{"location":"#core-concepts","title":"\ud83e\udde0 Core Concepts","text":"<p>MagicFlow is structured around two primary components:</p>"},{"location":"#jobs","title":"Jobs","text":"<p>A job is a single, atomic task. Jobs can be composed into workflows to execute higher-order automation processes. Examples of job tasks include:</p> <ul> <li>Creating a pull request in GitLab</li> <li>Verifying approval status before proceeding</li> <li>Deploying resources in cloud environments (e.g., GCP, AWS)</li> <li>Managing secrets in systems like Kubernetes, Vault, or AWS SSM</li> </ul>"},{"location":"#workflows","title":"Workflows","text":"<p>A workflow is a sequence of jobs that represent a complete operational process. Workflows enable structured automation of common scenarios such as:</p> <ul> <li>Tenant provisioning</li> <li>Resource quota adjustments or scaling</li> <li>Backup validation during disaster recovery (DR) testing</li> <li>Invoking DR procedures</li> <li>Generating and collecting reports</li> </ul>"},{"location":"#messaging-with-kafka","title":"\ud83d\udd04 Messaging with Kafka","text":"<p>MagicFlow uses Apache Kafka as its messaging backbone. It:</p> <ul> <li>Consumes job definitions and execution requests via Kafka topics</li> <li>Publishes job execution statuses and workflow outcomes back to Kafka</li> </ul> <p>This decoupled architecture enables event-driven automation and seamless integration with external orchestration systems or service portals.</p>"},{"location":"#integration","title":"\ud83d\udd17 Integration","text":"<p>MagicFlow is designed with modularity in mind. It can be:</p> <ul> <li>Embedded into CI/CD pipelines</li> <li>Triggered via REST APIs or Kafka events</li> <li>Used in conjunction with infrastructure-as-code or service management platforms</li> </ul>"},{"location":"#next-steps","title":"\ud83d\udcd8 Next Steps","text":"<p>Continue exploring the documentation to learn more about:</p> <ul> <li>Job Definitions</li> <li>Workflow Configuration</li> <li>Platform Integration</li> <li>Examples &amp; Use Cases</li> </ul>"},{"location":"examples/","title":"Job examples","text":"<ul> <li>Gitlab Pipeline - work with Gitlab pipeline<ul> <li>job: \"check_pipeline_status\" - Monitors the status of a GitLab merge request pipeline, pausing the workflow if the pipeline is still running or waiting, and returning the final status when complete.</li> <li>job: \"check_pipeline_job_status\" - Tracks the status of specific jobs within a GitLab pipeline, pausing the workflow if jobs are still running, and returning the final job status when complete.</li> <li>job: \"play_pipeline_job\" - Triggers a manual job in a GitLab pipeline and returns the job details and status after execution.</li> </ul> </li> <li>Secret Manager - work with secret manager GCP and AWS<ul> <li>job: \"ssm_list_all\" - Lists all parameters/secrets from AWS SSM Parameter Store or GCP Secret Manager for a given cloud, stage, and environment.</li> <li>job: \"ssm_list\" - Retrieves specific parameters/secrets from AWS SSM Parameter Store or GCP Secret Manager based on namespace and name.</li> <li>job: \"ssm_update\" - Updates existing parameters/secrets in AWS SSM Parameter Store or GCP Secret Manager with new values provided in YAML format.</li> </ul> </li> </ul>"},{"location":"examples/pipeline/","title":"Gitlab Pipeline Example","text":"<p>Place file in jobs lib with all required jobs(functions with decorator @). All jobs will be loaded on worker pod start </p> <pre><code>magicflow/\n\u2514\u2500\u2500 jobs/\n    \u2514\u2500\u2500 lib/\n        \u251c\u2500\u2500 dummy.py\n        \u2514\u2500\u2500 ssm.py\n        \u2514\u2500\u2500 pipeline.py &lt;---\n</code></pre> <pre><code>import time\nfrom magicflow.messaging import CommandMessage\nfrom magicflow.libs.gitlab_service import GitlabDriver\nfrom magicflow.libs.logging_service import LoggingService\nfrom magicflow.config.config import settings\nfrom magicflow.jobs import jobs, Jobs\n\nlogger = LoggingService().getLogger(__name__)\nj = jobs()\n\n@j.register(\"check_pipeline_status\")\ndef check_pipeline_status(context: Jobs, cmd: CommandMessage):\n    try:\n        input = cmd.get_data()[\"input\"]\n        if input[\"mr_created\"] == \"False\" or input[\n                \"expect_pipeline\"] == \"False\":\n            return {\"result\": \"No pipelines to check\"}\n\n        assert input[\"mr_id\"]\n        mr_id = input[\"mr_id\"]\n        assert input[\"project_id\"]\n        project_id = input[\"project_id\"]\n        expect_pipeline = input[\"expect_pipeline\"]\n\n        gl = GitlabDriver(settings.gitlab_api_token, project_id)\n        time.sleep(10)\n        pipelines = gl.check_mr_pipeline_status(mr_id)\n        logger.debug(f' Pipeline status: {pipelines}')\n        if pipelines:\n            for p in pipelines:\n                # The status of pipelines, one of:\n                # created, waiting_for_resource, preparing, pending,\n                # running, success, failed, canceled, skipped, manual, scheduled\n                # ref: https://docs.gitlab.com/ee/api/pipelines.html\n                pid = p[\"id\"]\n                purl = p[\"url\"]\n                pstatus = p[\"status\"]\n                if p[\"status\"] in [\n                        \"created\", \"waiting_for_resource\", \"preparing\",\n                        \"pending\", \"running\"\n                ]:\n                    return {\n                        \"result\": f'Pipeline [#{pid}]({purl}) {pstatus}',\n                        \"pipeline\": p,\n                        \"workflow_control\": {\n                            \"pause\": \"true\",\n                            \"pause_seconds\": 10\n                        }\n                    }\n                else:\n                    return {\n                        \"result\": f'Pipeline [#{pid}]({purl}) {pstatus}',\n                        \"parameters\": {\n                            \"pipeline_id\": str(p['id']),\n                            \"project_id\": project_id\n                        },\n                        \"pipeline\": p\n                    }\n        else:\n            if expect_pipeline == \"True\":\n                return {\n                    \"result\": f'Pipeline expected but not scheduled ',\n                    \"workflow_control\": {\n                        \"pause\": \"true\",\n                        \"pause_seconds\": 20\n                    }\n                }\n            else:\n                return {\"result\": f' No Pipelines expected'}\n    except KeyError:\n        logger.warning('Missing input parameters')\n        return {\"result\": f' No Pipelines found'}\n\n\n@j.register(\"check_pipeline_job_status\")\ndef check_pipeline_job_status(context: Jobs, cmd: CommandMessage):\n    try:\n        input = cmd.get_data()[\"input\"]\n        assert input[\"pipeline_job_id\"]\n        pipeline_job_id = input[\"pipeline_job_id\"]\n        assert input[\"project_id\"]\n        project_id = input[\"project_id\"]\n        assert input[\"pipeline_id\"]\n        pipeline_id = input[\"pipeline_id\"]\n\n        gl = GitlabDriver(settings.gitlab_api_token, project_id)\n        time.sleep(10)\n        jobs = gl.check_pipeline_job_status(pipeline_job_id)\n        logger.debug(f' JOBS status: {jobs}')\n        if jobs:\n            for _job in jobs:\n                logger.debug(f' JOB status: {_job}')\n                jid = _job[\"id\"]\n                jurl = _job[\"url\"]\n                jstatus = _job[\"status\"]\n                if _job[\"status\"] in [\n                        \"created\", \"waiting_for_resource\", \"preparing\",\n                        \"pending\", \"running\"\n                ]:\n                    return {\n                        \"result\": f'Job [#{jid}]({jurl}) {jstatus}',\n                        \"pipeline_job\": _job,\n                        \"workflow_control\": {\n                            \"pause\": \"true\",\n                            \"pause_seconds\": 10\n                        }\n                    }\n                else:\n                    return {\n                        \"result\": f'Job [#{jid}]({jurl}) {jstatus}',\n                        \"parameters\": {\n                            \"pipeline_id\": pipeline_id,\n                            \"pipeline_job_id\": str(_job['id']),\n                            \"project_id\": project_id\n                        },\n                        \"pipeline_job\": _job\n                    }\n        else:\n            return {\"result\": f' No Jobs found'}\n    except KeyError:\n        logger.error('Missing input parameters')\n        return {\"result\": f' No Job found'}\n\n\n@j.register(\"play_pipeline_job\")\ndef play_pipeline_job(context: Jobs, cmd: CommandMessage):\n    try:\n        input = cmd.get_data()[\"input\"]\n        assert input[\"project_id\"]\n        project_id = input[\"project_id\"]\n        assert input[\"pipeline_id\"]\n        pipeline_id = input[\"pipeline_id\"]\n        gl = GitlabDriver(settings.gitlab_api_token, project_id)\n        time.sleep(10)\n        play_job = gl.play_pipeline_job(pipeline_id)\n        if play_job:\n            play_job.update({\"project_id\": project_id})\n            if play_job[\"pipeline_job_id\"]:\n                return {\n                    \"parameters\":\n                    play_job,\n                    \"result\":\n                    f'Job [#{play_job[\"pipeline_job_id\"]}]({play_job[\"web_url\"]}) played'\n                }\n            else:\n                return {\"parameters\": play_job, \"result\": 'Failed to play job'}\n        else:\n            return {\"result\": f' No Pipelines found'}\n    except AssertionError:\n        logger.error('Missing input parameters')\n        return {\"result\": f' No Jobs found'}\n    except KeyError:\n        return {\"result\": f' No Jobs to check found'}\n</code></pre> <p>example workflow to import in to workflow controller. Construct workflow from multiple jobs, where you can pass one job output to another job input by reference it </p> <p><pre><code>            \"name\": \"check_pipeline_status\",\n            \"order\": 6,\n            \"input\": {\n                \"from_jobs\": [1],\n                \"expect_pipeline\": \"True\"\n            },\n</code></pre> as per above example you pass to job order 6 inputs from 1st job output and additionaly adding extra input parameter <code>expect_pipeline</code></p> <pre><code>{\n    \"name\": \"Work With Gitlab Example\",\n    \"template\": \"true\",\n    \"description\": \"Provision resource in new environment\",\n    \"category\": \"Provisioning\",\n    \"icon\": \"app.png\",\n    \"type\": \"Some service\",\n    \"parameters\": [\n        {\n            \"Name\": \"stage\",\n            \"value\": \"dev\",\n            \"description\": \"Stage name (stage)\"\n        },\n        {\n            \"Name\": \"environment\",\n            \"value\": \"development\",\n            \"description\": \"Environment name (environment)\"\n        },\n        {\n            \"Name\": \"namespace\",\n            \"value\": \"test\",\n            \"description\": \"Name of the namespace on which a new service will be provisioned (namespace)\"\n        },\n        {\n            \"Name\": \"instance\",\n            \"value\": \"mysuperapp\",\n            \"description\": \"Service instance name (instance)\"\n        },\n        {\n            \"Name\": \"cloud_provider\",\n            \"value\": \"gcp\",\n            \"description\": \"Cloud provider (cloud_provider)\"\n        },\n        {\n            \"Name\": \"jira_ticket\",\n            \"value\": \"SD-XXXX\",\n            \"description\": \"JIRA ticket number which to add to commit messages\"\n        }\n    ],\n    \"jobs\": [\n        {\n            \"name\": \"check_environment_exists\",\n            \"order\": 1,\n            \"input\": {\n                \"stage\": \"\",\n                \"environment\": \"\",\n                \"namespace\": \"\",\n                \"instance\": \"\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"create_namespace\",\n            \"order\": 2,\n            \"input\": {\n                \"stage\": \"\",\n                \"environment\": \"\",\n                \"namespace\": \"\",\n                \"service\": \"someapp\",\n                \"instance\": \"\",\n                \"jira_ticket\": \"\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"check_pipeline_status\",\n            \"order\": 3,\n            \"input\": {\n                \"from_jobs\": [1],\n                \"namespace\": \"\",\n                 \"expect_pipeline\": \"True\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"check_approval\",\n            \"order\": 4,\n            \"input\": {\n                \"from_jobs\": [1]\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"create_mr_nonprod\",\n            \"order\": 5,\n            \"input\": {\n                \"stage\": \"\",\n                \"environment\": \"\",\n                \"namespace\": \"\",\n                \"service\": \"someapp\",\n                \"instance\": \"\",\n                \"cloud_provider\": \"\",\n                \"jira_ticket\": \"\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"check_pipeline_status\",\n            \"order\": 6,\n            \"input\": {\n                \"from_jobs\": [4],\n                 \"expect_pipeline\": \"True\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"check_approval\",\n            \"order\": 7,\n            \"input\": {\n                \"from_jobs\": [4]\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },     \n        {\n            \"name\": \"merge\",\n            \"order\": 8,\n            \"input\": {\n                \"from_jobs\": [1]\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"merge\",\n            \"order\": 9,\n            \"input\": {\n                \"from_jobs\": [4]\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },           \n    ]\n}\n</code></pre>"},{"location":"examples/ssm/","title":"SSM (Systems Manager) Operations Module","text":"<p>This module provides a set of functions for interacting with cloud provider's Secret Management Systems (AWS SSM Parameter Store and GCP Secret Manager).</p> <pre><code>import zlib\nimport yaml\nimport json\n\nfrom magicflow.messaging import CommandMessage\nfrom magicflow.jobs import jobs, Jobs\nfrom magicflow.jobs.utils import validate_inputs, workflow_success, workflow_fail\nfrom magicflow.libs.logging_service import LoggingService\n\nlogger = LoggingService().getLogger(__name__)\n\nj = jobs()\n\n@j.register(\"ssm_list_all\")\ndef ssm_list_all(context: Jobs, cmd: CommandMessage):\n\n    metadata = context.metadata()\n\n    if metadata.validate([\"cloud\", \"stage\", \"environment\"]):\n        return workflow_fail(**metadata.validate([\"cloud\", \"stage\", \"environment\"]))\n\n    cloud = metadata.get(\"cloud\")\n    stage = metadata.get(\"stage\")\n    environment = metadata.get(\"environment\")\n\n    parameters = []\n    if cloud == \"aws\":\n        logger.debug(\"Working with AWS SSM\")\n        from magicflow.libs.ssm.aws import list_parameters\n        parameters = list_parameters(stage, environment)\n        if not parameters or len(parameters) == 0:\n            return workflow_success(\"No secrets found\")\n    elif cloud == \"gcp\":\n        logger.debug(\"Working with GCP SSM\")\n        from magicflow.libs.ssm.gcp import list_parameters\n        parameters = list_parameters(stage, environment)\n        if not parameters or len(parameters) == 0:\n            return workflow_success(\"No secrets found\")\n    else:\n        return workflow_fail(f'Cloud {cloud} not supported')\n\n    logger.debug(f\"Secrets found: {parameters}\")\n\n    return workflow_success(\"Secrets found\", logs=json.dumps(parameters, indent=4))\n\n\n@j.register(\"ssm_list\")\ndef ssm_list(context: Jobs, cmd: CommandMessage):\n\n    metadata = context.metadata()\n\n    if metadata.validate([\"cloud\", \"stage\", \"environment\"]):\n        return workflow_fail(**metadata.validate([\"cloud\", \"stage\", \"environment\"]))\n\n    if validate_inputs([\"namespace\", \"name\"], cmd):\n        return workflow_fail(**validate_inputs([\"namespace\", \"name\"], cmd))\n\n    namespace = cmd.get_data()[\"input\"][\"namespace\"]\n    name = cmd.get_data()[\"input\"][\"name\"]\n\n    cloud = metadata.get(\"cloud\")\n    stage = metadata.get(\"stage\")\n    environment = metadata.get(\"environment\")\n\n    parameters = {}\n    if cloud == \"aws\":\n        logger.debug(\"Working with AWS SSM\")\n        from magicflow.libs.ssm.aws import get_parameter\n        parameters = get_parameter(stage, environment, namespace, name)\n        if not parameters or len(parameters) == 0:\n            return workflow_success(\"No secrets found\")\n    elif cloud == \"gcp\":\n        logger.debug(\"Working with GCP SSM\")\n        from magicflow.libs.ssm.gcp import get_parameter\n        parameters = get_parameter(stage, environment, namespace, name)\n        if not parameters or len(parameters) == 0:\n            return workflow_success(\"No secrets found\")\n    else:\n        return workflow_fail(f'Cloud {cloud} not supported')\n\n    logger.debug(f\"Secrets found: {parameters}\")\n    output = {}\n    for key, value in parameters.items():\n        output[key] = f'crc32({zlib.crc32(str(value).encode()):x})'\n\n    return workflow_success(\"Secrets found\", logs=json.dumps(output, indent=4))\n\n@j.register(\"ssm_update\")\ndef ssm_update(context: Jobs, cmd: CommandMessage):\n    metadata = context.metadata()\n\n    if metadata.validate([\"cloud\", \"stage\", \"environment\"]):\n        return workflow_fail(**metadata.validate([\"cloud\", \"stage\", \"environment\"]))\n\n    if validate_inputs([\"namespace\", \"name\", \"values\"], cmd):\n        return workflow_fail(**validate_inputs([\"namespace\", \"name\", \"values\"], cmd))\n\n    namespace = cmd.get_data()[\"input\"][\"namespace\"]\n    name = cmd.get_data()[\"input\"][\"name\"]\n    values = yaml.safe_load(cmd.get_data()[\"input\"][\"values\"])\n\n    cloud = metadata.get(\"cloud\")\n    stage = metadata.get(\"stage\")\n    environment = metadata.get(\"environment\")\n\n    if cloud == \"aws\":\n        from magicflow.libs.ssm.aws import update_parameter\n        update_parameter(values, stage, environment, namespace, name)\n    elif cloud == \"gcp\":\n        from magicflow.libs.ssm.gcp import update_parameter\n        update_parameter(values, stage, environment, namespace, name)\n    else:\n        return workflow_fail(f'Cloud {cloud} not supported')\n    output = {}\n    for key, value in values.items():\n        output[key] = f'crc32({zlib.crc32(str(value).encode()):x})'\n\n    return workflow_success(\"Secret updated\", logs=json.dumps(output, indent=4))\n</code></pre>"},{"location":"framework/components/","title":"Components","text":""},{"location":"framework/components/#events","title":"Events","text":""},{"location":"framework/components/#general-rules-for-message-schema-formatting","title":"General Rules for Message Schema Formatting","text":"<ul> <li>Use <code>underscore_case</code> for attribute names.</li> <li>Use lowercase for attribute names, except when referencing an object \u2014 in that case, use CapitalCase for the key.</li> <li>Example: <code>\"attribute\": \"some_string\"</code> vs <code>\"Attribute\": {}</code></li> </ul>"},{"location":"framework/components/#standard-event-object-structure","title":"Standard Event Object Structure","text":"<p>Each event object must include:</p> <ul> <li><code>id</code>: Unique identifier of the message.</li> <li><code>Attributes</code>: Object containing header-like metadata:</li> <li><code>source</code>: The origin of the event (e.g., service name)</li> <li><code>type</code>: Event type for identification</li> <li><code>date</code>: Timestamp of the event</li> <li><code>Data</code>: Contains parameters intended for the event receiver (e.g., result of a command, or input parameters for a worker)</li> <li><code>message</code>: (Optional) Human-readable message for debugging or clarity</li> </ul>"},{"location":"framework/components/#command","title":"Command","text":"<p>A command is an object placed into a queue system to instruct the worker to perform an action.</p>"},{"location":"framework/components/#supported-commands","title":"Supported Commands","text":"Command Description Schema Example <code>RunJob</code> Executes the specified job using the worker engine <pre><code>{\n  \"id\": \"xxx-x-x-x-xxx-x-x-x-xx\",\n  \"Attributes\": {\n    \"source\": \"magicflow-controller\",\n    \"type\": \"RunJob\",\n    \"date\": \"2025-05-15T09:55:28.81Z\"\n  },\n  \"Data\": {\n    \"name\": \"create_check_namespace\",\n    \"workflow_id\": \"xxx-xxxx-xxxx\",\n    \"Input\": {\n      \"namespace_name\": \"poc-db\"\n    },\n    \"Metrics\": \"&lt;MetricsObject&gt;\"\n  }\n}\n</code></pre>"},{"location":"framework/components/#events_1","title":"Events","text":"Event Description Schema Example <code>RunJobCompleted</code> Indicates successful job completion <pre><code>{\n  \"id\": \"xxx-x-x-x-xxx-x-x-x-xx\",\n  \"Attributes\": {\n    \"source\": \"magicflow-worker\",\n    \"type\": \"RunJobCompleted\",\n    \"date\": \"2025-05-15T09:55:28.81Z\"\n  },\n  \"message\": \"RunJobCompleted completed successfully\",\n  \"Data\": {\n    \"name\": \"create_check_namespace\",\n    \"workflow_id\": \"xxxx-xxxx-xxx\",\n    \"Input\": \"&lt;CommandObject&gt;\",\n    \"Output\": \"&lt;OutputObject&gt;\",\n    \"Metrics\": \"&lt;MetricsObject&gt;\"\n  }\n}\n</code></pre> <code>RunJobFailed</code>  Indicates job failure <pre><code>{\n  \"id\": \"xxx-x-x-x-xxx-x-x-x-xx\",\n  \"attributes\": {\n    \"source\": \"magicflow-worker\",\n    \"type\": \"RunJobFailed\",\n    \"date\": \"2025-05-15T09:55:28.81Z\"\n  },\n  \"message\": \"Something went wrong\",\n  \"Data\": {\n    \"name\": \"create_check_namespace\",\n    \"workflow_id\": \"xxxx-xxxx-xxx\",\n    \"Input\": \"&lt;CommandObject&gt;\",\n    \"error\": \"SomeError\",\n    \"Stack\": \"&lt;StackTrace&gt;\",\n    \"status\": \"500\",\n    \"Metrics\": \"&lt;MetricsObject&gt;\"\n  }\n}\n</code></pre> <code>WorkflowStarted</code>  Indicates the start of a workflow <pre><code>{\n  \"id\": \"xxx-x-x-x-xxx-x-x-x-xx\",\n  \"attributes\": {\n    \"source\": \"magicflow-controller\",\n    \"type\": \"WorkflowStarted\",\n    \"date\": \"2025-05-15T09:55:28.81Z\"\n  },\n  \"message\": \"Started workflow\",\n  \"Data\": {\n    \"workflow_id\": \"xxxx-xxxx-xxx\",\n    \"Metrics\": \"&lt;MetricsObject&gt;\"\n  }\n}\n</code></pre> <code>WorkflowCompleted</code>  Indicates successful completion of a workflow <pre><code>{\n  \"id\": \"xxx-x-x-x-xxx-x-x-x-xx\",\n  \"attributes\": {\n    \"source\": \"magicflow-controller\",\n    \"type\": \"WorkflowCompleted\",\n    \"date\": \"2025-05-15T09:55:28.81Z\"\n  },\n  \"message\": \"Workflow completed\",\n  \"Data\": {\n    \"workflow_id\": \"xxxx-xxxx-xxx\",\n    \"Metrics\": \"&lt;MetricsObject&gt;\",\n    \"Output\": \"&lt;OutputObject&gt;\"\n  }\n}\n</code></pre> <code>WorkflowFailed</code>  Indicates a workflow has failed <pre><code>{\n  \"id\": \"xxx-x-x-x-xxx-x-x-x-xx\",\n  \"attributes\": {\n    \"source\": \"magicflow-controller\",\n    \"type\": \"WorkflowFailed\",\n    \"date\": \"2025-05-15T09:55:28.81Z\"\n  },\n  \"message\": \"Something went wrong\",\n  \"Data\": {\n    \"workflow_id\": \"xxxx-xxxx-xxx\",\n    \"error\": \"SomeError\",\n    \"Stack\": \"&lt;StackTrace&gt;\",\n    \"status\": \"500\",\n    \"Metrics\": \"&lt;MetricsObject&gt;\"\n  }\n}\n</code></pre>"},{"location":"framework/components/#event-status","title":"Event Status","text":""},{"location":"framework/components/#runjobfailed","title":"RunJobFailed","text":"Status Code Description 501 Validation error 500 Internal error"},{"location":"framework/components/#workflowfailed","title":"WorkflowFailed","text":"Status Code Description 400 Not authorized 500 Internal error"},{"location":"framework/components/#metricsobject","title":"MetricsObject","text":"<p>The MetricsObject is used to append service-related timestamp checkpoints for calculating in-service and end-to-end (E2E) metrics. Each \"step\" in a process can push a checkpoint to the <code>MetricsObject</code>, enabling complete traceability.</p> Stack Position (Index) Checkpoint Name Timestamp (s) 0 api_workflow_requested 100 1 controller_command_sent 101 2 worker_message_received 110 3 worker_started_job 111 4 worker_job_completed 119 5 controller_event_received 125 6 controller_workflow_completed 130 <p>From this stack, metrics like E2E duration can be calculated: <code>controller_workflow_completed - api_workflow_requested = 30s</code></p>"},{"location":"framework/jobs/","title":"Job Structure","text":"<pre><code>flowchart TD\n    subgraph Workflow_Controller\n        WF[Workflow]\n        JobDef[\"Job (Definition)\"]\n        WF --&gt; JobDef\n        note1[\"Job is defined as JSON (inputs/outputs)\"]\n        JobDef --&gt; note1\n    end\n\n    subgraph MagicFlow_Worker\n        JobCode[\"Job (Implementation)\"]\n        note2[\"Job implemented as Python code in /jobs/lib\"]\n        JobCode --&gt; note2\n    end\n\n    JobDef -- \"executed by\" --&gt; JobCode</code></pre>"},{"location":"framework/jobs/#workflow-controller-api","title":"workflow-controller API","text":"<p>Define wokflow that will be imported in to workflow-controller API by POST request.  <code>category</code> and <code>type</code> not play any critical role in execution or provisioning . </p> <p><code>parameters</code> are fields you expect some value to add </p> <p></p> <pre><code># Post workflow templates\nfor i in $DIRECTORY/workflow-*.json; do\n    # Process $i\n    curl -XPUT $API_URL/workflow-templates --header \"Content-Type: application/json\" --header \"Authorization: Bearer $API_TOKEN\" -d @$i\ndone\n</code></pre> <p>example workflow JSON </p> <pre><code>{\n    \"name\": \"SSM update workflow\",\n    \"template\": \"true\",\n    \"description\": \"To update/create secrets for an application\",\n    \"category\": \"Provisioning\",\n    \"type\": \"Secrets\",\n\n    \"parameters\": [\n        {\n            \"name\": \"namespace\",\n            \"value\": \"\",\n            \"description\": \"Namespace of an application\"\n        },\n        {\n            \"name\": \"name\",\n            \"value\": \"\",\n            \"description\": \"Name of an application\"\n        },\n        {\n          \"name\": \"Environment\",\n          \"type\": \"target-environment\",\n          \"value\": \"\",\n          \"description\": \"Target environment to run workflow on\"\n      },        \n        {\n            \"name\": \"values\",\n            \"type\": \"yaml\",\n            \"value\": \"SOME: \\\"Value\\\"\\r\\nBLAH: \\\"Value2\\\"\",\n            \"description\": \"Values to be updated/created in SSM\"\n        },\n        {\n            \"name\": \"target-environment\",\n            \"value\": \"\",\n            \"type\": \"target-environment\",\n            \"description\": \"Name of the environment\"\n        }\n    ],\n    \"jobs\": [\n        {\n            \"name\": \"ssm_update\",\n            \"description\": \"Update/create SSM secrets for an application\",\n            \"order\": 0,\n            \"controller\": \"\",\n            \"input\": {\n                \"values\": \"\",\n                \"name\": \"\",\n                \"namespace\": \"\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        }\n    ]\n}\n</code></pre> <p>job <code>name</code> shoudl be = function name in magicflow-worker <code>jobs/lib</code> . <code>@j.register(\"ssm_update\")</code></p> <pre><code>magicflow/\n\u2514\u2500\u2500 jobs/\n    \u2514\u2500\u2500 lib/\n        \u251c\u2500\u2500 dummy.py\n        \u2514\u2500\u2500 ssm.py\n</code></pre>"},{"location":"framework/jobs/#magicflow-worker","title":"magicflow-worker","text":"<p>Service where your actual python code jobs are deployed in <code>jobs/lib</code> path as per above .  if we have multiple jobs in workflow, controller will convert etch job in to kafka message adding attributes with workflow_id </p> <pre><code>    \"jobs\": [{\n            \"name\": \"dummy_job\",\n            \"description\": \"Dummy job. Does nothing.\",\n            \"order\": 0,\n            \"controller\": \"\",\n            \"input\": {\n                \"mr_id\": \"\",\n                \"start_date\": \"\",\n                \"features\": \"\",\n                \"incident_description\": \"\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        },\n        {\n            \"name\": \"dummy_job\",\n            \"description\": \"Dummy job. Does nothing.\",\n            \"order\": 0,\n            \"controller\": \"\",\n            \"input\": {\n                \"mr_id\": \"\"\n            },\n            \"output\": {\n                \"error\": {}\n            },\n            \"status\": \"New\"\n        }\n    ]\n</code></pre>"},{"location":"framework/testing/","title":"Testing localy","text":""},{"location":"framework/testing/#preprequisites","title":"Preprequisites","text":"<ul> <li>kind  </li> <li>devspace </li> </ul>"},{"location":"framework/testing/#local-kubernetes-cluster","title":"Local kubernetes cluster","text":"<p>Use <code>kind</code> to deploy local kubernetes cluster on your pc </p> <pre><code>kind create cluster --name dev-magicflow\n</code></pre> <p>connect to it via context</p> <pre><code>kubectl cluster-info --context kind-dev-magicflow\n</code></pre> Spin up local k8s cluster    ### Connect to it <pre><code> $ kind create cluster --name dev-magicflow\nCreating cluster \"dev-magicflow\" ...\n \u2713 Ensuring node image (kindest/node:v1.33.1) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-dev-magicflow\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-dev-magicflow\n\nHave a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community \ud83d\ude42\n~/CTO2B/github/magicflow [feat/devspace] $ kubectl cluster-info --context kind-dev-magicflow\nKubernetes control plane is running at https://127.0.0.1:50072\nCoreDNS is running at https://127.0.0.1:50072/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"framework/testing/#deploy-test-env","title":"Deploy test env","text":"<ul> <li>make sure you have Preprequisites and you are in magicflow dir <code>~/CTO2B/magicflow[main] $</code></li> <li>run devspace</li> </ul> <pre><code>devspace dev -n magicflow\n</code></pre> <p>NOTE:  above comand may fail by run it 1st time due to CRD's deploy take some time</p> Check Deploy Output    ### Start Dev Env  ``` ~/CTO2B/magicflow [main]$ devspace dev -n magicflow info Using namespace 'magicflow' info Using kube context 'kind-dev-magicflow' info Created namespace: magicflow pullsecret:gitlab-registry Ensuring image pull secret for registry: registry.gitlab.com... pullsecret:gitlab-registry Created image pull secret magicflow/registry-credentials deploy:strimzi Deploying chart oci://quay.io/strimzi-helm/strimzi-kafka-operator (strimzi) with helm... deploy:kafka-debug Deploying chart  (kafka-debug) with helm... deploy:kafka Deploying chart  (kafka) with helm... deploy:crds Deploying chart  (crds) with helm... deploy:magicflow Deploying chart  (magicflow) with helm... deploy:kafka-debug Deployed helm chart (Release revision: 1) deploy:kafka-debug Successfully deployed kafka-debug with helm create_deployments: error deploying magicflow: unable to deploy helm chart: Release \"magicflow\" does not exist. Installing it now.  error executing 'helm upgrade magicflow --values /var/folders/9f/k4x9t2xs321f39rzxnd985_h0000gn/T/2026060257 --install --namespace magicflow /Users/andrius/CTO2B/github/magicflow/k8s/magicflow --kube-context kind-dev-magicflow': Error: unable to build kubernetes objects from release manifest: resource mapping not found for name: \"super-user\" namespace: \"magicflow\" from \"\": no matches for kind \"KafkaUser\" in version \"kafka.strimzi.io/v1beta2\" ensure CRDs are installed first  fatal exit status 1 ~/CTO2B/magicflow [main]$ devspace dev -n magicflow info Using namespace 'magicflow' info Using kube context 'kind-dev-magicflow' pullsecret:gitlab-registry Ensuring image pull secret for registry: registry.gitlab.com... deploy:strimzi Deploying chart oci://quay.io/strimzi-helm/strimzi-kafka-operator (strimzi) with helm... deploy:kafka Deploying chart  (kafka) with helm... deploy:crds Deploying chart  (crds) with helm... deploy:kafka-debug Deploying chart  (kafka-debug) with helm... deploy:magicflow Deploying chart  (magicflow) with helm... deploy:kafka Deployed helm chart (Release revision: 1) deploy:kafka Successfully deployed kafka with helm deploy:magicflow Deployed helm chart (Release revision: 1) deploy:magicflow Successfully deployed magicflow with helm deploy:kafka-debug Deployed helm chart (Release revision: 2) deploy:kafka-debug Successfully deployed kafka-debug with helm deploy:crds Deployed helm chart (Release revision: 2) deploy:crds Successfully deployed crds with helm deploy:strimzi Deployed helm chart (Release revision: 1) deploy:strimzi Successfully deployed strimzi with helm dev:magicflow Waiting for pod to become ready... dev:magicflow Pod magicflow-devspace-99d478fcd-brlbx: MountVolume.SetUp failed for volume \"ca\" : secret \"kafka-client-cluster-ca-cert\" not found (FailedMount) dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: Init:0/1 dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow DevSpace is waiting, because Pod magicflow-devspace-99d478fcd-brlbx has status: PodInitializing dev:magicflow Selected pod magicflow-devspace-99d478fcd-brlbx dev:magicflow sync  Sync started on: magicflow/ &lt;-&gt; /magicflow/ dev:magicflow sync  Waiting for initial sync to complete dev:magicflow sync  Initial sync completed   ```"},{"location":"framework/testing/#deployment-status","title":"deployment status","text":"<ul> <li>entity operator for user management</li> <li>client-0 Kafka cluster one node</li> <li>kafka-debug pod for debuging kafka</li> <li>magicflow worker (job processor)</li> <li>strimzi kafka operator </li> </ul> <pre><code>$ k get po -n magicflow\nNAME                                          READY   STATUS    RESTARTS   AGE\nkafka-client-entity-operator-cc6b69f6-2khft   1/1     Running   0          8m52s\nkafka-client-kafka-client-0                   1/1     Running   0          9m45s\nkafka-debug-54445c74c6-q5dk9                  1/1     Running   0          25s\nmagicflow-devspace-99d478fcd-brlbx            1/1     Running   0          10m\nstrimzi-cluster-operator-6f4fc4667c-4r4hj     1/1     Running   0          10m\n</code></pre>"},{"location":"framework/testing/#testing","title":"Testing","text":"<p>NOTE:  devspace will sync all your changes in code to container. If your code require new modules image rebuild required. Refer to devspace build manual </p> Get Report (Optional)    ### Check topics and consumers  <pre><code>k exec -it kafka-debug-54445c74c6-q5dk9 -n magicflow -- /bin/bash\nDefaulted container \"kafka-debug\" out of: kafka-debug, kafka-truststore (init)\nkafka-debug-54445c74c6-q5dk9:/apps/kafka_2.13-3.9.1/bin# ./kafka_debug.sh -a -g\nSasl: 1\nStarting debug in kafka-client-kafka-bootstrap:9093 | Output file: /tmp/kafka_debug_log.txt\n###################### Getting all Kafka topics...   ###################\ncto2b.magicflow.workflows.worker.job cto2b.magicflow.workflows.worker.status\n###################### Getting consumer groups...    ###################\ncto2b.magicflow.group1\n###################### Getting all group...          ###################\n\nGROUP                  TOPIC                                PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                  HOST            CLIENT-ID\ncto2b.magicflow.group1 cto2b.magicflow.workflows.worker.job 0          -               0               -               rdkafka-e4d54869-a367-43bd-a12e-62d6dc3f2c8a /10.244.0.7     rdkafka###################### Getting all groups members... ###################\n\nGROUP                  CONSUMER-ID                                  HOST            CLIENT-ID       #PARTITIONS\ncto2b.magicflow.group1 rdkafka-e4d54869-a367-43bd-a12e-62d6dc3f2c8a /10.244.0.7     rdkafka         1\n###################### Getting topics description... ###################\nTopic: __consumer_offsets   TopicId: Lrw71VxQQHmLdjzEMx2TKw PartitionCount: 50  ReplicationFactor: 1    Configs: compression.type=producer,min.insync.replicas=1,cleanup.policy=compact,flush.ms=240000,segment.bytes=104857600,unclean.leader.election.enable=false\n    Topic: __consumer_offsets   Partition: 0    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 1    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 2    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 3    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 4    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 5    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 6    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 7    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 8    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 9    Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 10   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 11   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 12   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 13   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 14   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 15   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 16   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 17   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 18   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 19   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 20   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 21   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 22   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 23   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 24   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 25   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 26   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 27   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 28   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 29   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 30   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 31   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 32   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 33   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 34   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 35   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 36   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 37   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 38   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 39   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 40   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 41   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 42   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 43   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 44   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 45   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 46   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 47   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 48   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\n    Topic: __consumer_offsets   Partition: 49   Leader: 0   Replicas: 0 Isr: 0  Elr:    LastKnownElr:\nTopic: cto2b.magicflow.workflows.worker.job TopicId: 4L6Gc_BQT52_5yQpQVFGFQ PartitionCount: 1   ReplicationFactor: 1    Configs: min.insync.replicas=1,flush.ms=240000,unclean.leader.election.enable=false\n    Topic: cto2b.magicflow.workflows.worker.job Partition: 0    Leader: 0   Replicas: 0 Isr: 0  Elr: LastKnownElr:\nTopic: cto2b.magicflow.workflows.worker.status  TopicId: cu56fOokRrSUri4X-PNxxQ PartitionCount: 1   ReplicationFactor: 1    Configs: min.insync.replicas=1,flush.ms=240000,unclean.leader.election.enable=false\n    Topic: cto2b.magicflow.workflows.worker.status  Partition: 0    Leader: 0   Replicas: 0 Isr: 0  Elr: LastKnownElr:\n###################### End of report...###################\n</code></pre> Publish Test job MSG    ### Save Job For Publishing <pre><code>kafka-debug-54445c74c6-q5dk9:/apps/kafka_2.13-3.9.1/bin# cat &gt; job1\n{\"id\":\"b8e4e91f-3a71-46af-9f91-f01999c80734\",\"created_at\":\"0001-01-01T00:00:00Z\",\"updated_at\":\"0001-01-01T00:00:00Z\",\"attributes\":{\"source\":\"dbot-controller\",\"type\":\"job-input\",\"date\":\"2025-05-15T09:55:28.81Z\"},\"message\":\"\",\"data\":{\"created_at\":\"2025-05-15T09:55:28.81Z\",\"input\":{\"from_jobs\":[1],\"mr_id\":\"1121\",\"project_id\":\"25047365\"},\"job_id\":\"66288897-7c32-44ba-9d38-0f8aa1ba944a\",\"name\":\"dummy_job\",\"order\":3,\"output\":{\"error\":{}},\"status\":\"New\",\"updated_at\":\"2025-05-15T09:55:28.81Z\",\"workflow_id\":\"d2981e4e-b0fc-46be-90ac-4c5b17b1cb0b\"}}\n</code></pre>   ### Publish saved job  <pre><code>kafka-debug-54445c74c6-q5dk9:/apps/kafka_2.13-3.9.1/bin# cat job1 | ./kafka-console-producer.sh   --topic cto2b.magicflow.workflows.worker.job   --bootstrap-server $KAFKA_BOOTSTRAP_SERVERS   --producer.config ../config/client_sasl.properties\n</code></pre> &gt; **_NOTE:_**  Json should be oneliner for publishing    ### Json example  <pre><code>{\n    \"id\": \"b8e4e91f-3a71-46af-9f91-f01999c80734\",\n    \"created_at\": \"0001-01-01T00:00:00Z\",\n    \"updated_at\": \"0001-01-01T00:00:00Z\",\n    \"attributes\": {\n        \"source\": \"dbot-controller\",\n        \"type\": \"job-input\",\n        \"date\": \"2025-05-15T09:55:28.81Z\"\n    },\n    \"message\": \"\",\n    \"data\": {\n        \"created_at\": \"2025-05-15T09:55:28.81Z\",\n        \"input\": {\n            \"from_jobs\": [\n                1\n            ],\n            \"mr_id\": \"1121\",\n            \"project_id\": \"25047365\"\n        },\n        \"job_id\": \"66288897-7c32-44ba-9d38-0f8aa1ba944a\",\n        \"name\": \"dummy_job\",\n        \"order\": 3,\n        \"output\": {\n            \"error\": {}\n        },\n        \"status\": \"New\",\n        \"updated_at\": \"2025-05-15T09:55:28.81Z\",\n        \"workflow_id\": \"d2981e4e-b0fc-46be-90ac-4c5b17b1cb0b\"\n    }\n}\n</code></pre> Job Process Output    ### Log from magicflow <pre><code>2025-05-20 18:49:18,713 - app.__name__ - INFO - KAFKA-SASL: Received message chunk: {\"id\":\"b8e4e91f-3a71-46af-9f91-f01999c80734\",\"created_at\":\"0001-01-01T00:00:00Z\",\"updated_at\":\"0001-01-01T00:00:00Z\",\"attributes\":{\"source\":\"dbot-controller\",\"type\":\"job-input\",\"date\":\"2025-05-15T09:55:28.81Z\"},\"message\":\"\",\"data\":{\"created_at\":\"2025-05-15T09:55:28.81Z\",\"input\":{\"from_jobs\":[1],\"mr_id\":\"1121\",\"project_id\":\"25047365\"},\"job_id\":\"66288897-7c32-44ba-9d38-0f8aa1ba944a\",\"name\":\"dummy_job\",\"order\":3,\"output\":{\"error\":{}},\"status\":\"New\",\"updated_at\":\"2025-05-15T09:55:28.81Z\",\"workflow_id\":\"d2981e4e-b0fc-46be-90ac-4c5b17b1cb0b\"}}\n\n2025-05-20 18:49:18,724 - app.magicflow.jobs - DEBUG - Got command with id b8e4e91f-3a71-46af-9f91-f01999c80734\n\n2025-05-20 18:49:18,725 - app.dummy - DEBUG - Doing dummy job msg = &lt;magicflow.messaging.CommandMessage object at 0x7ffffe63fe80&gt;\n\n2025-05-20 18:49:18,744 - app.__name__ - INFO - Producing message to topic cto2b.magicflow.workflows.worker.status: {\"id\": \"b8e4e91f-3a71-46af-9f91-f01999c80734\", \"created_at\": \"0001-01-01T00:00:00Z\", \"updated_at\": \"0001-01-01T00:00:00Z\", \"attributes\": {\"source\": \"dbot-controller\", \"type\": \"job-input\", \"date\": \"2025-05-15T09:55:28.81Z\"}, \"message\": \"\", \"data\": {\"created_at\": \"2025-05-15T09:55:28.81Z\", \"input\": {\"from_jobs\": [1], \"mr_id\": \"1121\", \"project_id\": \"25047365\"}, \"job_id\": \"66288897-7c32-44ba-9d38-0f8aa1ba944a\", \"name\": \"dummy_job\", \"order\": 3, \"output\": {\"error\": {}, \"mr_id:\": \"1121\"}, \"status\": \"Completed\", \"updated_at\": \"2025-05-15T09:55:28.81Z\", \"workflow_id\": \"d2981e4e-b0fc-46be-90ac-4c5b17b1cb0b\"}}\n\n2025-05-20 18:49:18,828 - app.__name__ - INFO - KAFKA-SASL: Message delivered to cto2b.magicflow.workflows.worker.status [0] at offset 0\n\n2025-05-20 18:49:18,829 - app.__name__ - INFO - Message successfully produced to topic cto2b.magicflow.workflows.worker.status\n</code></pre>  we see status topic updated with `Completed` status. our job get input and pass input to output so we see output  `output\": {\"error\": {}, \"mr_id:\": \"1121\"}`"},{"location":"framework/testing/#cleanup","title":"Cleanup","text":"<pre><code>$ kind delete cluster -n dev-magicflow\nDeleting cluster \"dev-magicflow\" ...\nDeleted nodes: [\"dev-magicflow-control-plane\"]\n</code></pre>"},{"location":"integration/architecture/","title":"Architecture","text":""},{"location":"integration/architecture/#glossary","title":"Glossary","text":"Term Definition Workflow An end-to-end (E2E) process that executes a defined sequence of operations, conceptually similar to a GitLab pipeline. Key characteristics:\u2022 Unidirectional execution flow\u2022 Each step (a job or group of jobs) is triggered only after the previous step completes successfully\u2022 Rollback is not currently supported\u2022 Steps are idempotent\u2014restarting a step produces the same result Workflow Step A discrete unit of work such as \"create merge request\", or a logical grouping of independent jobs. Workflow Job A function that performs a specific operation. Jobs accept input parameters and return output results. Command A JSON-based message representing an instruction to execute a specific job with specific parameters. Note: The presence of a command does not guarantee that the job has been executed. Event A JSON-based message containing the result of a job execution. Note: The presence of an event confirms that the job has successfully completed. Workflow Controller A SAGA-like service that implements the business logic of the workflow. It manages the state and orchestrates the flow by translating events into commands."},{"location":"integration/architecture/#integration-architecture-example","title":"Integration Architecture Example","text":"<p>The diagram above illustrates a remote job execution architecture.</p> <p>Within Google Cloud, microservices are deployed alongside a Workflow Controller\u2014an API with state management capabilities. This controller is responsible for:</p> <ul> <li>Managing and storing active workflow instances</li> <li>Receiving job status updates</li> <li>Orchestrating the sequence of job executions</li> </ul> <p>This enables asynchronous, event-driven execution of workflows across distributed services.</p>"}]}